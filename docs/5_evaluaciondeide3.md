La evaluación de IDE de IDERA
=============================

Identificación de oportunidades de mejora
-----------------------------------------

La Evaluación de IDE de IDERA, tal como ha sido formulada originalmente,
presenta algunos aspectos muy positivos, como permitir cierta flexibilidad en la
inclusión/modificación de indicadores y la posibilidad de desarrollar diferentes
versiones (acorde con los avances de las IDE en la aplicación de los estándares
y de IDERA en cuanto a la formulación de nuevas recomendaciones).

También resulta valiosa como contribución, si se piensa a la evaluación como
herramienta para la mejora y, en la misma línea, se la enlaza con acciones tales
como el desarrollo de capacitaciones personalizadas, acordes a las diversas
necesidades específicas de las distintas IDE que forman parte de IDERA.

Para los propios organismos a cargo de las diferentes IDE que integran IDERA, la
posibilidad de autoevaluación que surge de la metodología de evaluación de IDE
desarrollada por IDERA, también ofrece una serie de beneficios.

Así, a partir de dicha autoevaluación, es posible detectar, en primer lugar y en
forma sistemática, cuáles son los principales problemas que presenta la IDE en
un determinado momento.

A su vez, sobre la base de dicho diagnóstico periódico, facilitado por las
herramientas que brinda la autoevaluación, es que los organismos pueden tomar
decisiones más informadas respecto a la priorización de esfuerzos en
determinadas áreas, servicios y procesos, de cara al fortalecimiento de sus IDE.

Dichos esfuerzos incluyen cuestiones tales como la revisión de procedimientos,
la optimización del hardware y del software y la capacitación del personal en
las diversas tareas que hacen al funcionamiento de la IDE.

Para desarrollar y aprovechar mejor todas estas potencialidades de la evaluación
de IDE de IDERA, es necesario señalar algunos problemas que presenta. Así, al
momento de su formulación, se pensaba trabajar en ciertos aspectos que todavía
no se han concretado, tales como la ausencia de algunos estándares que sustentan
determinados indicadores de la evaluación, que aún no han sido formulados. De
allí la necesidad de adaptar la metodología de evaluación a lo que realmente
pueden hacer las IDE del país, en función de los estándares efectivamente
aprobados por IDERA.

Surge entonces la necesidad de realizar una serie de preguntas en relación al
efectivo funcionamiento de la Evaluación de IDE, dado que, tras la aprobación de
su metodología por parte del equipo coordinador de IDERA y su testeo en
diferentes oportunidades, se hicieron cada vez más visibles sus falencias y la
necesidad de elaborar de herramientas o precisiones que faciliten su
implementación.

### ¿Cómo se implementa la Evaluación de IDE de IDERA?

En el documento del procedimiento de evaluación (documento de trabajo que hasta
el momento no fue publicado) se plantean una serie de pasos para el proceso de
evaluación, así como una estructura de responsables para su implementación. Sin
embargo, en la práctica, a pesar de que se hicieron algunas pruebas piloto,
nunca se llevó a cabo el procedimiento de manera completa, e inclusive se ha
discontinuado el seguimiento de los nodos IDE y el relevamiento de información
periódica sobre su funcionamiento.

A su vez, hasta el momento no se implementó la evaluación, ya que es posible que
dicha implementación haya resultado una carga excesiva para la Coordinación
Ejecutiva de IDERA en los últimos años. Ello es visible en el hecho de que la 
Evaluación de IDE implica un esfuerzo desmedido para la Coordinación Ejecutiva de 
IDERA, dado que en la práctica, la experiencia recorrida muestra que para funcionar 
adecuadamente requiere de cierta masa crítica de personal calificado y dedicado a 
esta tarea, en un contexto de achicamiento del Estado y de restricciones para la 
contratación de personal, sumado a un consenso de los actores, que, si bien 
teóricamente se tiene, no se termina de concretar.

Además, el esquema generado de variables e indicadores no ha resultado homogéneo
en términos de su simplicidad y economía de recursos para el relevamiento de los
respectivos datos, dado que subsisten demasiados procedimientos que implican
mucho trabajo difícil de automatizar. Por ejemplo, la revisión de nombres y
metadatos en el caso de la existencia de miles de capas, como en la IDET (para
la que se hizo la prueba piloto), resulta excesiva como tarea de revisión de
cada una para una sola persona y vuelve al procedimiento de evaluación muy
costoso, engorroso e impreciso. En otros casos, algunas variables resultaron de
muy compleja medición, tal como en lo que respecta a la salud de los servidores
y a la aplicación de los estándares de estilos.

La prueba piloto demostró que la retroalimentación recibida en la instancia de
revisión por parte de miembros del Grupo Coordinador fue muy pobre. Es de
destacar que el trabajo voluntario, por más que esté enmarcado en instituciones
u organismos a los cuales pertenecen los representantes, tiene la falencia de la
imposibilidad de asignar plazos y obligatoriedad a la realización de las tareas.

Del mismo modo, han surgido muchas dudas entre los responsables de las IDE, en
referencia a la forma de completar el informe descriptivo de la IDE durante la
prueba piloto, por lo que su metodología de elaboración es pasible de mejoras
tales como, por ejemplo, la realización de entrevistas entre el equipo de apoyo
a la Evaluación de IDE de la Coordinación Ejecutiva de IDERA y los actores, para
realizar su elaboración en forma conjunta.

### ¿La metodología de evaluación de IDERA permite conocer el grado de funcionamiento efectivo de una IDE?

Si bien la prueba piloto permitió comprobar la eficacia de la Evaluación de IDE
para detectar que las recomendaciones y estándares de IDERA no estaban siendo
aplicados en forma parcial o total por las IDE evaluadas, una de las grandes
preguntas que no se logró responder con la aplicación de estas variables e
indicadores es si la IDE en cuestión, en los hechos, se encontraba funcionando
plenamente.

Se considera como IDE operativa a la que se encuentra generando y publicando
nuevas capas de información geográfica, actualizando las existentes, manteniendo
actualizado y completo el catálogo de metadatos y funcionando como una
comunidad, con plena participación de sus integrantes en la toma de decisiones
de la IDE.

En general, las iniciativas IDE se lanzan con mucha energía y recursos, y, a
partir de la implementación de paquetes de software logran comenzar a funcionar,
pero luego no les es posible perdurar en el tiempo, ya sea por la escasez de
recursos económicos, por la movilidad del principal recurso humano que las lleva
adelante, o por la pérdida de apoyo político.

Así, los ciclos de vida de las IDE, al depender de los apoyos políticos, de los 
recursos humanos y de la situación fiscal y macroeconómica, hacen que presenten 
muchas fluctuaciones en el tiempo, tanto ante cambios en el signo político del 
gobierno, como frente a modificaciones en los equilibrios internos entre los 
distintos grupos que integran una coalición gobernante. Ejemplos de esto lo 
constituyen la IDE de la provincia de Río Negro, creada a través de los 
Decretos N° 1839 y 1846 del año 2013, que debido a un quiebre en la coalición 
interna de gobierno, dejó de funcionar en el año 2015 y hasta la fecha no registra 
nueva actividad. Algo similar ocurre con la IDE de la provincia de San Luis, 
creada a través del Decreto N° 7372 del año 2012, que en el año 2017, debido a 
modificaciones en la conformación del gabinete del gobierno provincial, quedó sin 
funcionamiento. Con lo cual puede pensarse que si bien la existencia de normativa 
de creación de una IDE es un requisito necesario para su funcionamiento y 
legitimación, no necesariamente garantiza su sostenibilidad a lo largo del tiempo, 
inclusive en el marco de una misma gestión de gobierno, lo cual tampoco asegura su 
operatividad.

Todas estas cuestiones no son, actualmente, captadas en forma adecuada por los
indicadores y el procedimiento establecidos para la Evaluación de IDE. Ya que no
basta con que una IDE esté funcionando para que efectivamente cumpla su rol de
ser de utilidad para la sociedad. Es por esto que resulta complejo encontrar
variables que realmente respondan a la pregunta sobre la operatividad de una
IDE, debido a que no existe un standard ni recomendaciones al respecto en la
bibliografía analizada, por lo que es muy difícil de medir. Se torna
imprescindible, entonces, avanzar en este tema en conjunto con los actores en
IDERA a fin de consensuar una definición respecto de qué se entiende por la
operatividad de una IDE, de la cual surjan indicadores para medirla.

### ¿Cuentan los organismos con información clara y concreta acerca de los estándares de IDERA que deberían cumplir?

En gran medida, las variables consideradas en la Evaluación de IDE no solo se
basan en los estándares generados por IDERA, sino que también remiten a las
normas internacionales y a las condiciones básicas para permitir el
funcionamiento de los elementos de la IDE.

Así, al momento de diseñar e implementar la Evaluación de IDE fue posible
verificar la existencia de una serie de elementos cuyos estándares no habían
sido todavía definidos por escrito por IDERA en sus recomendaciones e
instructivos.

De esta forma, muchas cuestiones se daban por sentadas o por sabidas. Una
muestra de ello es el caso del Grupo de Trabajo de Metadatos de IDERA, que
generó a fines de 2017 un documento de Buenas Prácticas que aclara diversas
dudas y pone por escrito muchas definiciones y procedimientos que antes se daban
por sentadas, por ejemplo, en relación a cómo utilizar el linaje para la
documentación de capas en los metadatos.

Hay otras variables que se considera necesario evaluar, dado que existen
estándares de IDERA publicados, pero cuya implementación todavía no está
ampliamente difundida o que los actores no consideran prioritaria, tales como el
Catálogo de Objetos Geográficos o la discriminación de datos básicos y
fundamentales. En este último caso, todavía no hay normativa de IDERA respecto a
quien es el responsable principal de la producción y la publicación de cada uno
de los datos básicos y fundamentales.

Por otro lado, subsisten una serie de falencias en relación con la publicación
de esta documentación y normas por parte de IDERA. Si bien el sitio web de IDERA
ha ido mejorando considerablemente con el tiempo, todavía resulta muy complejo
encontrar los documentos necesarios a la hora de ver cómo solucionar los
problemas detectados por los indicadores de la Evaluación de IDE. Ello puede
obedecer a que la recomendación específica se pierde dentro de un documento de
una gran extensión, a que dicho documento todavía no existe, a que existe pero
se encuentra desactualizado, a que la página no presenta una forma sencilla,
amigable e intuitiva para acceder a dicho recurso o, tal como se ha referido
para el caso de los documentos de la Evaluación de IDE (procedimiento, esquema
de variables, recomendaciones para geoportal y estructura del informe) han sido
aprobados por el Equipo Coordinador de IDERA, pero, por inconsistencias en el
proceso de publicación, no se encuentran disponibles en su sitio web.

Siendo la evaluación una herramienta tan esencial para la evolución de las IDE y
para facilitar la labor de la Coordinación Ejecutiva de IDERA, el sentido del
presente análisis crítico ha sido apuntalar y revitalizar su funcionamiento,
atento a lo cual, y tomando como referencias las falencias detectadas, se ha
elaborado una propuesta de mejoras para su implementación, pensadas como aportes
que buscan contribuir al fortalecimiento de IDERA.
